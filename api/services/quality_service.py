"""
Quality Service - Story 4 Business Logic
Location: api/services/quality_service.py

Layer 1: get_summary() - Quality issues summary
Layer 2: get_operation_detail() - Operation calls with quality breakdown
Layer 3: Shared CallDetail (uses llm_call_service.py)
"""

from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict
from datetime import datetime
from observatory.storage import ObservatoryStorage

# =============================================================================
# CONSTANTS & THRESHOLDS
# =============================================================================

THRESHOLDS = {
    "quality_critical": 5.0,    # Below this = critical
    "quality_low": 7.0,         # Below this = low quality
    "quality_good": 8.0,        # Above this = good
    "error_rate_critical": 5.0, # Above 5% = critical
    "error_rate_warning": 2.0,  # Above 2% = warning
}


# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

def _get_quality_score(call) -> Optional[float]:
    """Extract quality score from call (handles both dict and object)."""
    # Try direct attribute first (new extracted column)
    if hasattr(call, 'judge_score') and call.judge_score is not None:
        return call.judge_score
    
    # Try dict access
    if isinstance(call, dict):
        if 'judge_score' in call and call['judge_score'] is not None:
            return call['judge_score']
        
        # Fallback to nested quality_evaluation
        if 'quality_evaluation' in call and call['quality_evaluation']:
            qe = call['quality_evaluation']
            if isinstance(qe, dict) and 'judge_score' in qe:
                return qe['judge_score']
    
    # Try nested quality_evaluation object
    if hasattr(call, 'quality_evaluation') and call.quality_evaluation:
        if hasattr(call.quality_evaluation, 'judge_score'):
            return call.quality_evaluation.judge_score
    
    return None


def _get_hallucination_flag(call: Dict) -> bool:
    """Check if call has hallucination flag."""
    # Try extracted column first
    if call.get("hallucination_flag") is not None:
        return bool(call["hallucination_flag"])
    
    # Try quality_evaluation JSON
    quality = call.get("quality_evaluation") or {}
    if isinstance(quality, dict):
        return bool(quality.get("hallucination_flag", False))
    
    return False


def _get_error_category(call: Dict) -> Optional[str]:
    """Get error category from call data."""
    # Try extracted column first
    if call.get("error_category"):
        return call["error_category"]
    
    # Try quality_evaluation JSON
    quality = call.get("quality_evaluation") or {}
    if isinstance(quality, dict) and quality.get("error_category"):
        return quality["error_category"]
    
    # Fallback to error_type
    return call.get("error_type")


def _get_quality_status(score: Optional[float]) -> Tuple[str, str, str]:
    """
    Returns (status, emoji, label) for quality score.
    """
    if score is None:
        return ("unknown", "âšª", "Not Evaluated")
    if score < THRESHOLDS["quality_critical"]:
        return ("critical", "ðŸ”´", "Critical")
    if score < THRESHOLDS["quality_low"]:
        return ("low", "ðŸ”´", "Low Quality")
    if score < THRESHOLDS["quality_good"]:
        return ("ok", "ðŸŸ¡", "Acceptable")
    return ("good", "ðŸŸ¢", "Good")


def _get_error_rate_status(error_rate: float) -> Tuple[str, str]:
    """Returns (status, emoji) for error rate."""
    if error_rate >= THRESHOLDS["error_rate_critical"]:
        return ("critical", "ðŸ”´")
    if error_rate >= THRESHOLDS["error_rate_warning"]:
        return ("warning", "ðŸŸ¡")
    return ("healthy", "ðŸŸ¢")


def _format_timestamp(ts: Any) -> str:
    """Format timestamp for display."""
    if not ts:
        return "â€”"
    
    if isinstance(ts, str):
        try:
            ts = datetime.fromisoformat(ts.replace("Z", "+00:00"))
        except:
            return ts[:16] if len(ts) > 16 else ts
    
    if isinstance(ts, datetime):
        return ts.strftime("%b %d, %I:%M %p")
    
    return str(ts)


def _build_quality_histogram(scores: List[float], bucket_size: float = 1.0) -> List[Dict]:
    """Build quality score histogram."""
    if not scores:
        return []
    
    # Create buckets (0-1, 1-2, ..., 9-10)
    buckets = defaultdict(int)
    for score in scores:
        bucket = int(score)  # Floor to nearest integer
        bucket = min(bucket, 9)  # Cap at 9 (9-10 bucket)
        buckets[bucket] += 1
    
    # Return all buckets from 0 to 10
    result = []
    for i in range(11):
        count = buckets.get(i, 0)
        if i < 10:
            label = f"{i}-{i+1}"
        else:
            label = "10"
        
        # Determine color based on threshold
        if i < 5:
            color = "critical"
        elif i < 7:
            color = "low"
        elif i < 8:
            color = "ok"
        else:
            color = "good"
        
        result.append({
            "bucket": i,
            "label": label,
            "count": count,
            "color": color,
        })
    
    # Filter to only buckets with data or adjacent to data
    min_bucket = min(buckets.keys()) if buckets else 0
    max_bucket = max(buckets.keys()) if buckets else 10
    
    return [r for r in result if min_bucket - 1 <= r["bucket"] <= max_bucket + 1]


# =============================================================================
# LAYER 1: SUMMARY
# =============================================================================

def get_summary(calls: List[Dict], project: str = None, days: int = 7) -> Dict:
    """
    Layer 1: Quality issues summary.
    
    Returns summary KPIs, operations table, and chart data.
    """
    # Filter to only LLM calls with tokens
    llm_calls = [c for c in calls if (c.get("prompt_tokens") or 0) > 0]
    
    if not llm_calls:
        return {
            "status": "ok",
            "health_score": 100.0,
            "summary": {
                "total_calls": 0,
                "evaluated_calls": 0,
                "avg_quality": None,
                "avg_quality_formatted": "â€”",
                "low_quality_count": 0,
                "error_count": 0,
                "error_rate": 0.0,
                "error_rate_formatted": "0%",
                "hallucination_count": 0,
            },
            "top_offender": None,
            "detail_table": [],
            "chart_data": [],
        }
    
    # Global metrics
    all_scores = []
    total_errors = 0
    total_hallucinations = 0
    
    for call in llm_calls:
        score = _get_quality_score(call)
        if score is not None:
            all_scores.append(score)
        
        if not call.get("success", True):
            total_errors += 1
        
        if _get_hallucination_flag(call):
            total_hallucinations += 1
    
    avg_quality = sum(all_scores) / len(all_scores) if all_scores else None
    error_rate = (total_errors / len(llm_calls) * 100) if llm_calls else 0
    
    # Group by operation
    by_operation = defaultdict(list)
    for call in llm_calls:
        agent = call.get("agent_name") or "Unknown"
        op = call.get("operation") or "unknown"
        by_operation[(agent, op)].append(call)
    
    detail_table = []
    low_quality_ops = []
    
    for (agent, op), op_calls in by_operation.items():
        # Calculate metrics
        scores = [_get_quality_score(c) for c in op_calls]
        scores = [s for s in scores if s is not None]
        
        avg_score = sum(scores) / len(scores) if scores else None
        min_score = min(scores) if scores else None
        max_score = max(scores) if scores else None
        
        error_count = sum(1 for c in op_calls if not c.get("success", True))
        halluc_count = sum(1 for c in op_calls if _get_hallucination_flag(c))
        
        # Determine status
        status, status_emoji, status_label = _get_quality_status(avg_score)
        
        # Track low quality operations
        if avg_score is not None and avg_score < THRESHOLDS["quality_low"]:
            low_quality_ops.append((agent, op, avg_score, op_calls))
        
        detail_table.append({
            "agent_name": agent,
            "operation_name": op,
            "status": status,
            "status_emoji": status_emoji,
            "status_label": status_label,
            "avg_score": avg_score,
            "avg_score_formatted": f"{avg_score:.1f}/10" if avg_score else "â€”",
            "min_score": min_score,
            "min_score_formatted": f"{min_score:.1f}" if min_score else "â€”",
            "max_score": max_score,
            "max_score_formatted": f"{max_score:.1f}" if max_score else "â€”",
            "error_count": error_count,
            "hallucination_count": halluc_count,
            "call_count": len(op_calls),
            "evaluated_count": len(scores),
        })
    
    # Sort by avg_score (worst first), then by error count
    detail_table.sort(key=lambda x: (
        x["avg_score"] if x["avg_score"] is not None else 999,
        -x["error_count"]
    ))
    
    # Top offender (worst quality operation)
    top_offender = None
    if low_quality_ops:
        low_quality_ops.sort(key=lambda x: x[2])  # Sort by avg_score ascending
        agent, op, score, op_calls = low_quality_ops[0]
        
        error_count = sum(1 for c in op_calls if not c.get("success", True))
        halluc_count = sum(1 for c in op_calls if _get_hallucination_flag(c))
        
        issues = []
        if error_count > 0:
            issues.append(f"{error_count} errors")
        if halluc_count > 0:
            issues.append(f"{halluc_count} hallucinations")
        
        top_offender = {
            "agent": agent,
            "operation": op,
            "avg_score": score,
            "avg_score_formatted": f"{score:.1f}/10",
            "call_count": len(op_calls),
            "error_count": error_count,
            "hallucination_count": halluc_count,
            "diagnosis": f"Avg quality {score:.1f}/10" + (f" with {', '.join(issues)}" if issues else ""),
        }
    
    # Chart data - quality distribution
    chart_data = _build_quality_histogram(all_scores)
    
    # Calculate health score
    if avg_quality is None:
        health_score = 100.0
        status = "ok"
    elif avg_quality < THRESHOLDS["quality_critical"]:
        health_score = max(20, avg_quality * 10)
        status = "error"
    elif avg_quality < THRESHOLDS["quality_low"]:
        health_score = max(40, 50 + (avg_quality - 5) * 10)
        status = "warning"
    elif avg_quality < THRESHOLDS["quality_good"]:
        health_score = max(70, 70 + (avg_quality - 7) * 15)
        status = "ok"
    else:
        health_score = min(100, 85 + (avg_quality - 8) * 7.5)
        status = "ok"
    
    # Adjust for error rate
    if error_rate >= THRESHOLDS["error_rate_critical"]:
        health_score = max(30, health_score - 30)
        status = "error"
    elif error_rate >= THRESHOLDS["error_rate_warning"]:
        health_score = max(50, health_score - 15)
        if status == "ok":
            status = "warning"
    
    error_status, error_emoji = _get_error_rate_status(error_rate)
    
    return {
        "status": status,
        "health_score": round(health_score, 1),
        "summary": {
            "total_calls": len(llm_calls),
            "evaluated_calls": len(all_scores),
            "avg_quality": avg_quality,
            "avg_quality_formatted": f"{avg_quality:.1f}/10" if avg_quality else "â€”",
            "low_quality_count": len(low_quality_ops),
            "error_count": total_errors,
            "error_rate": round(error_rate, 1),
            "error_rate_formatted": f"{error_rate:.1f}%",
            "error_rate_status": error_status,
            "error_rate_emoji": error_emoji,
            "hallucination_count": total_hallucinations,
        },
        "top_offender": top_offender,
        "detail_table": detail_table,
        "chart_data": chart_data,
    }


# =============================================================================
# LAYER 2: OPERATION DETAIL
# =============================================================================

def get_operation_detail(
    calls: List[Dict],
    agent: str,
    operation: str
) -> Optional[Dict]:
    """
    Layer 2: Operation detail with calls and quality distribution.
    
    Returns detailed view of a single operation including:
    - Summary stats (avg/min/max score, errors, hallucinations)
    - All calls with quality info
    - Quality score histogram
    """
    # Filter calls for this operation
    op_calls = [
        c for c in calls
        if c.get("agent_name") == agent and c.get("operation") == operation
    ]
    
    if not op_calls:
        return None  # 404
    
    # Calculate aggregates
    scores = []
    total_errors = 0
    total_hallucinations = 0
    
    for call in op_calls:
        score = _get_quality_score(call)
        if score is not None:
            scores.append(score)
        
        if not call.get("success", True):
            total_errors += 1
        
        if _get_hallucination_flag(call):
            total_hallucinations += 1
    
    avg_score = sum(scores) / len(scores) if scores else None
    min_score = min(scores) if scores else None
    max_score = max(scores) if scores else None
    
    status, status_emoji, status_label = _get_quality_status(avg_score)
    
    # Build calls list (sorted by score, worst first)
    calls_list = []
    sorted_calls = sorted(
        op_calls,
        key=lambda x: (_get_quality_score(x) if _get_quality_score(x) is not None else 999)
    )
    
    for i, call in enumerate(sorted_calls):
        score = _get_quality_score(call)
        is_error = not call.get("success", True)
        is_hallucination = _get_hallucination_flag(call)
        error_category = _get_error_category(call)
        
        # Build issues list
        issues = []
        if is_error:
            issues.append({"type": "error", "emoji": "âŒ", "label": "Error"})
        if is_hallucination:
            issues.append({"type": "hallucination", "emoji": "âš ï¸", "label": "Hallucination"})
        if error_category:
            issues.append({"type": "category", "emoji": "ðŸ“‹", "label": error_category})
        
        score_status, score_emoji, score_label = _get_quality_status(score)
        
        calls_list.append({
            "call_id": call.get("id"),
            "index": i + 1,
            "timestamp": call.get("timestamp"),
            "timestamp_formatted": _format_timestamp(call.get("timestamp")),
            "score": score,
            "score_formatted": f"{score:.1f}/10" if score is not None else "â€”",
            "score_status": score_status,
            "score_emoji": score_emoji,
            "is_error": is_error,
            "is_hallucination": is_hallucination,
            "issues": issues,
            "issues_display": " ".join([i["emoji"] for i in issues]) if issues else "",
            "latency_ms": call.get("latency_ms") or 0,
            "latency_formatted": f"{(call.get('latency_ms') or 0) / 1000:.1f}s",
            "total_cost": call.get("total_cost") or 0,
            "cost_formatted": f"${(call.get('total_cost') or 0):.4f}",
            "model_name": call.get("model_name") or "â€”",
            "prompt_preview": (call.get("prompt") or "")[:50] + "..." if call.get("prompt") else "â€”",
        })
    
    # Quality distribution histogram
    quality_distribution = _build_quality_histogram(scores)
    
    return {
        "agent_name": agent,
        "operation_name": operation,
        
        # Status badge
        "status": status,
        "status_emoji": status_emoji,
        "status_label": status_label,
        
        # Quality stats
        "avg_score": avg_score,
        "avg_score_formatted": f"{avg_score:.1f}/10" if avg_score else "â€”",
        "min_score": min_score,
        "min_score_formatted": f"{min_score:.1f}" if min_score else "â€”",
        "max_score": max_score,
        "max_score_formatted": f"{max_score:.1f}" if max_score else "â€”",
        
        # Counts
        "call_count": len(op_calls),
        "evaluated_count": len(scores),
        "error_count": total_errors,
        "hallucination_count": total_hallucinations,
        
        # Calls table
        "calls": calls_list,
        
        # Histogram
        "quality_distribution": quality_distribution,
    }

def get_quality_benchmarks(call_id: str) -> Dict[str, Any]:
    """Get quality comparison benchmarks for a call."""
    
    # Get current call
    current_call_obj = ObservatoryStorage.get_llm_call_by_id(call_id)
    
    if not current_call_obj:
        return {"available": False, "reason": "Call not found"}
    
    current_score = _get_quality_score(current_call_obj)
    
    if current_score is None:
        return {"available": False, "reason": "No quality score for this call"}
    
    if not current_call_obj.operation:
        return {"available": False, "reason": "No operation context"}
    
    # Get all calls for same operation (no time filter - respect frontend filter)
    try:
        operation_calls = ObservatoryStorage.get_llm_calls(
            agent_name=current_call_obj.agent_name,
            operation=current_call_obj.operation,
            limit=1000,  # Increased limit
        )
    except Exception as e:
        return {"available": False, "reason": f"Database error: {str(e)}"}
    
    if len(operation_calls) < 2:
        return {"available": False, "reason": "Not enough calls for comparison"}
    
    # Filter to calls with quality scores
    scored_calls = []
    for c in operation_calls:
        if c.id == call_id:
            continue
        score = _get_quality_score(c)
        if score is not None:
            scored_calls.append({
                "id": c.id,
                "score": score,
                "timestamp": c.timestamp,
            })
    
    if len(scored_calls) == 0:
        return {"available": False, "reason": "No other scored calls"}
    
    # Find highest score
    highest = max(scored_calls, key=lambda x: x["score"])
    
    # Calculate median
    all_scores = [c["score"] for c in scored_calls] + [current_score]
    all_scores.sort()
    median_score = all_scores[len(all_scores) // 2]
    
    return {
        "available": True,
        "current": {
            "call_id": call_id,
            "score": current_score,
            "operation": current_call_obj.operation,
        },
        "highest_same_operation": {
            "call_id": highest["id"],
            "score": highest["score"],
            "better_by": round(highest["score"] / current_score, 1) if current_score > 0 else 0,
            "timestamp": highest["timestamp"].isoformat() if highest["timestamp"] else None,
        },
        "median_for_operation": {
            "score": median_score,
            "comparison": "worse" if current_score < median_score else "better",
            "ratio": round(abs(current_score - median_score) / median_score, 1) if median_score > 0 else 0,
        },
        "total_calls_for_operation": len(scored_calls) + 1,
    }